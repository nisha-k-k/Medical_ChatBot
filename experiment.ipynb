{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Chatbot\n",
    "\n",
    "In this notebook, I will be creating a medical chatbot utilizing a Llama 2 quantized model. The model is linked below:\n",
    "\n",
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "\n",
    "with the specific id being:\n",
    "\n",
    "llama-2-7b-chat.Q4_K_M.gguf\n",
    "\n",
    "#### Data Used\n",
    "For this chatbot, I will be using PDF files containing medical book as data.\n",
    "\n",
    "Once we upload our data, we have to do the following on the backend:\n",
    "1. Extract the data/content\n",
    "2. Create chunks of text (because LLMs can only take in a certain amount of tokens at once. In Llama 2's case, 4096 tokens)\n",
    "- When creating chunks, we also create overlaps in the chunks so that the model consistently gets context\n",
    "3. Create text embeddings (ie, convert each text chunk into a vector of numbers that can be used as input into the model)\n",
    "4. Build semantic index and create knowledge base using a vector DB (Pinecone, for this project)\n",
    "\n",
    "\n",
    "After doing all the backend stuff above, we have to do the front-end stuff (how the app will interact with the user):\n",
    "1. User will ask a question/query\n",
    "2. The query will be embedded\n",
    "3. The query is sent to the knowledge base we have created on the backend in Pinecone\n",
    "4. The Knowledgge base will send back a ranked result, which is the closest vector with respect to the query you have asked\n",
    "5. The ranked result will be input into the LLM, and the LLM will send the response to the user\n",
    "\n",
    "#### Tech Stack\n",
    "Below is the tech stack we are going to use:\n",
    "1. Programming language --> Python\n",
    "2. Generattive AI (or LlamaIndex) Framework --> Langchain\n",
    "3. Frontend/Web App --> Flask\n",
    "4. LLM --> Meta Llama 2\n",
    "5. Vector DB --> Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medchat/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import ctransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
